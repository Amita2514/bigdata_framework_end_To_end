# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XFLpOWqeMCfQxYZxhub8wRsalmjKBf-i
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/app.py
# # ---- Streamlit + PySpark app (Colab-friendly) ----
# import streamlit as st
# st.set_page_config(page_title="NYC Taxi â€” Spark App", layout="wide")
# 
# import os, pandas as pd, numpy as np
# from math import radians, cos, sin, asin, sqrt
# 
# # Spark
# from pyspark.sql import SparkSession, functions as F
# from pyspark.sql.types import *
# from pyspark.sql.functions import udf, col
# 
# # ML
# from pyspark.ml import Pipeline
# from pyspark.ml.feature import VectorAssembler, Imputer
# from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor
# from pyspark.ml.classification import LogisticRegression
# from pyspark.ml.clustering import KMeans
# from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator
# from pyspark.mllib.evaluation import BinaryClassificationMetrics  # for ROC via RDD
# 
# # Viz
# import plotly.express as px
# import plotly.graph_objects as go
# import plotly.io as pio
# 
# 
# # ---------- Spark session (cached) ----------
# @st.cache_resource
# def get_spark():
#     return (SparkSession.builder
#             .appName("Streamlit+Spark")
#             .master("local[2]")
#             .config("spark.ui.enabled","false")
#             .config("spark.driver.bindAddress","127.0.0.1")
#             .config("spark.driver.host","127.0.0.1")
#             .config("spark.driver.port","30021")
#             .config("spark.blockManager.port","30022")
#             .config("spark.port.maxRetries","0")
#             .config("spark.local.dir","/content/spark-temp")
#             .getOrCreate())
# spark = get_spark()
# 
# 
# # ---------- Helpers ----------
# @udf("double")
# def haversine(lon1, lat1, lon2, lat2):
#     if None in (lon1, lat1, lon2, lat2): return None
#     lon1, lat1, lon2, lat2 = map(float, (lon1, lat1, lon2, lat2))
#     lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
#     dlon, dlat = lon2 - lon1, lat2 - lat1
#     a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2
#     return float(3958.8 * 2 * asin(sqrt(a)))  # miles
# 
# def engineer_features(df):
#     out = df
#     if out is None: return None
#     if "tpep_pickup_datetime" in out.columns:
#         out = out.withColumn("hour", F.hour("tpep_pickup_datetime")) \
#                  .withColumn("dow", F.dayofweek("tpep_pickup_datetime")) \
#                  .withColumn("is_weekend", F.when(F.col("dow").isin([1,7]), 1).otherwise(0))
#     if {"pickup_longitude","pickup_latitude","dropoff_longitude","dropoff_latitude"}.issubset(out.columns):
#         out = out.withColumn("geo_distance", haversine("pickup_longitude","pickup_latitude",
#                                                        "dropoff_longitude","dropoff_latitude"))
#     if {"tpep_pickup_datetime","tpep_dropoff_datetime"}.issubset(out.columns):
#         out = out.withColumn("duration_min",
#                 (F.unix_timestamp("tpep_dropoff_datetime") - F.unix_timestamp("tpep_pickup_datetime"))/60.0)
#     if {"geo_distance","duration_min"}.issubset(out.columns):
#         out = out.withColumn("speed_mph", F.col("geo_distance") / (F.col("duration_min")/60.0))
#     return out
# 
# def clean_numeric(df, cols):
#     out = df
#     for c in cols:
#         out = out.withColumn(c, F.col(c).cast(DoubleType()))
#     return out.replace(float("inf"), None).replace(float("-inf"), None)
# 
# def drop_if_exists(df, cols):
#     for c in cols:
#         if c in df.columns:
#             df = df.drop(c)
#     return df
# 
# def spark_head_pd(df, n=200):
#     return df.limit(n).toPandas()
# 
# def fig_to_html_bytes(fig):
#     html = pio.to_html(fig, include_plotlyjs="cdn", full_html=True)
#     return html.encode("utf-8")
# 
# def plot_roc_from_preds(preds_df, label_col, prob_col):
#     scores = preds_df.select(
#         F.col(prob_col).getItem(1).alias("score"),
#         F.col(label_col).cast("double").alias("label")
#     ).rdd.map(lambda r: (float(r["score"]), float(r["label"])))
#     metrics = BinaryClassificationMetrics(scores)
#     roc_pts = metrics.roc().collect()
#     roc_df = pd.DataFrame(roc_pts, columns=["FPR","TPR"])
#     return px.line(roc_df, x="FPR", y="TPR", title="ROC Curve")
# 
# 
# # ---------- UI ----------
# st.title("ðŸš– NYC Taxi â€” PySpark + Streamlit")
# 
# with st.sidebar:
#     st.header("Data Source")
#     src = st.radio("Choose source:", ["Upload CSV","Use existing Parquet"], index=1)
#     sample_frac = st.slider("Sample fraction", 0.05, 1.0, 0.30, 0.05)
#     if src == "Use existing Parquet":
#         parquet_path = st.text_input("Parquet directory", "/content/parquet/2015_01_clean")
#         load_btn = st.button("Load Parquet")
#     else:
#         up = st.file_uploader("Upload CSV", type=["csv"])
#         load_btn = st.button("Load CSV")
# 
# # Load data
# if "df" not in st.session_state:
#     st.session_state.df = None
# 
# if load_btn:
#     try:
#         if src == "Use existing Parquet":
#             st.session_state.df = spark.read.parquet(parquet_path)
#         else:
#             if up is None:
#                 st.warning("Upload a CSV first.")
#             else:
#                 tmp = "/content/uploads"; os.makedirs(tmp, exist_ok=True)
#                 pth = os.path.join(tmp, up.name)
#                 with open(pth, "wb") as f: f.write(up.getbuffer())
#                 st.session_state.df = (spark.read
#                                        .option("header","true")
#                                        .option("inferSchema","true")
#                                        .csv(pth))
#         if st.session_state.df is not None:
#             st.success("âœ… Data loaded.")
#     except Exception as e:
#         st.error(f"Load error: {e}")
# 
# df = st.session_state.df
# df_eng = engineer_features(df)
# if df_eng is not None:
#     df_eng = df_eng.sample(False, float(sample_frac), seed=42).cache()
# 
# # ---- 1) Preview & Summary ----
# st.subheader("ðŸ“„ Data Preview & Summary")
# if df_eng is None:
#     st.info("Load a dataset from the sidebar to get started.")
# else:
#     c1, c2 = st.columns([3,2])
#     with c1:
#         st.write("Top rows")
#         st.dataframe(spark_head_pd(df_eng, n=200))
#     with c2:
#         st.write({"rows": df_eng.count(), "columns": len(df_eng.columns)})
#         st.text("Schema:")
#         st.code("\n".join(df_eng._jdf.schema().treeString().splitlines()))
#         num_cols = [c for c,t in df_eng.dtypes if t in ("double","int","bigint")]
#         if num_cols:
#             st.write("Describe (numeric):")
#             try:
#                 st.dataframe(df_eng.select(num_cols).summary().toPandas())
#             except Exception:
#                 st.caption("Summary skipped (too large).")
# 
# # ---- 2) SQL Query Builder ----
# st.subheader("ðŸ§® Spark SQL â€” Query Builder")
# if df_eng is not None:
#     df_eng.createOrReplaceTempView("data")
#     default_sql = "SELECT hour, is_weekend, fare_amount, trip_distance, tip_amount FROM data LIMIT 10"
#     user_sql = st.text_area("SQL (table name = data)", value=default_sql, height=130)
#     if st.button("Run SQL"):
#         try:
#             qdf = spark.sql(user_sql)
#             st.dataframe(spark_head_pd(qdf, 500))
#             st.caption(f"Returned rows: {qdf.count()} (showing up to 500)")
#         except Exception as e:
#             st.error(f"SQL error: {e}")
# 
# # ---- 3) ML Module (Regression / Classification) ----
# st.subheader("ðŸ¤– ML Module")
# if df_eng is not None:
#     all_cols = df_eng.columns
#     numeric_cols = [c for c,t in df_eng.dtypes if t in ("double","int","bigint")]
#     problem = st.radio("Problem type:", ["Regression","Classification"], index=0, horizontal=True)
# 
#     # ---------- Regression ----------
#     if problem == "Regression":
#         target = st.selectbox("Target (numeric)",
#                               options=[c for c in numeric_cols if c != "high_tip"],
#                               index=(numeric_cols.index("fare_amount") if "fare_amount" in numeric_cols else 0) if numeric_cols else 0)
#         defaults = [c for c in ["geo_distance","duration_min","speed_mph","hour","is_weekend","passenger_count"] if c in all_cols]
#         feats = st.multiselect("Features", options=numeric_cols, default=defaults)
#         algo = st.selectbox("Algorithm", ["LinearRegression","DecisionTreeRegressor"], index=0)
#         test_size = st.slider("Test fraction", 0.1, 0.5, 0.2, 0.05)
# 
#         if st.button("Train regression"):
#             if not feats:
#                 st.warning("Select at least one feature.")
#             else:
#                 try:
#                     feats = [c for c in feats if c != target]  # drop leakage
#                     df_ml = clean_numeric(df_eng, list(set(feats + [target])))
#                     df_ml = drop_if_exists(df_ml, ["prediction"])  # avoid clashes from prior runs
#                     imputed = [f"{c}_imp" for c in feats]
# 
#                     pred_col = f"pred_{target}"
#                     reg_est = (LinearRegression(featuresCol="features", labelCol=target, predictionCol=pred_col, maxIter=50)
#                                if algo=="LinearRegression"
#                                else DecisionTreeRegressor(featuresCol="features", labelCol=target, predictionCol=pred_col, maxDepth=8))
# 
#                     pipe = Pipeline(stages=[
#                         Imputer(inputCols=feats, outputCols=imputed, strategy="median"),
#                         VectorAssembler(inputCols=imputed, outputCol="features"),
#                         reg_est
#                     ])
#                     data = df_ml.filter(col(target).isNotNull())
#                     train, test = data.randomSplit([1.0 - test_size, test_size], seed=42)
#                     fitted = pipe.fit(train)
#                     preds  = fitted.transform(test)
# 
#                     # Metrics
#                     rmse = RegressionEvaluator(labelCol=target, predictionCol=pred_col, metricName="rmse").evaluate(preds)
#                     r2   = RegressionEvaluator(labelCol=target, predictionCol=pred_col, metricName="r2").evaluate(preds)
#                     mae  = RegressionEvaluator(labelCol=target, predictionCol=pred_col, metricName="mae").evaluate(preds)
#                     st.success(f"RMSE: {rmse:.3f} | MAE: {mae:.3f} | R2: {r2:.3f}")
# 
#                     # Visuals
#                     ap_pd = preds.select(target, pred_col).limit(5000).toPandas()
#                     fig_ap = px.scatter(ap_pd, x=target, y=pred_col, title=f"Actual vs Predicted â€” {target}")
#                     st.plotly_chart(fig_ap, use_container_width=True)
#                     st.download_button("â¬‡ï¸ Chart (HTML): Actual vs Predicted", fig_to_html_bytes(fig_ap),
#                                        "actual_vs_predicted.html", "text/html")
# 
#                     preds = preds.withColumn("residual", F.col(pred_col) - F.col(target))
#                     res_pd = preds.select("residual").limit(5000).toPandas()
#                     fig_res = px.histogram(res_pd, x="residual", nbins=40, title="Residuals (prediction âˆ’ actual)")
#                     st.plotly_chart(fig_res, use_container_width=True)
#                     st.download_button("â¬‡ï¸ Chart (HTML): Residuals", fig_to_html_bytes(fig_res),
#                                        "residuals.html", "text/html")
# 
#                     last_stage = fitted.stages[-1]
#                     if isinstance(last_stage, DecisionTreeRegressor):
#                         importances = last_stage.featureImportances.toArray().tolist()
#                         fig_imp = px.bar(x=feats, y=importances, labels={"x":"feature","y":"importance"},
#                                          title="Decision Tree â€” Feature importance")
#                         st.plotly_chart(fig_imp, use_container_width=True)
#                         st.download_button("â¬‡ï¸ Chart (HTML): Importance", fig_to_html_bytes(fig_imp),
#                                            "importance.html", "text/html")
#                     elif isinstance(last_stage, LinearRegression):
#                         coefs = last_stage.coefficients.toArray().tolist()
#                         fig_coef = px.bar(x=feats, y=coefs, labels={"x":"feature","y":"coefficient"},
#                                           title="Linear Regression â€” Coefficients")
#                         st.plotly_chart(fig_coef, use_container_width=True)
#                         st.download_button("â¬‡ï¸ Chart (HTML): Coefficients", fig_to_html_bytes(fig_coef),
#                                            "coefficients.html", "text/html")
# 
#                     # Predictions table + download
#                     out_pd = preds.select(*feats, target, pred_col).limit(5000).toPandas()
#                     st.dataframe(out_pd)
#                     st.download_button("â¬‡ï¸ Predictions (CSV)",
#                                        out_pd.to_csv(index=False).encode(),
#                                        "regression_predictions.csv", "text/csv")
#                 except Exception as e:
#                     st.error(f"Regression error: {e}")
# 
#     # ---------- Classification ----------
#     else:
#         # auto-create label if possible
#         if "high_tip" not in all_cols and "tip_amount" in all_cols:
#             df_eng = df_eng.withColumn("high_tip", (F.col("tip_amount") >= F.lit(2.0)).cast("int"))
#             st.session_state.df = df_eng
#             all_cols = df_eng.columns
#             numeric_cols = [c for c,t in df_eng.dtypes if t in ("double","int","bigint")]
# 
#         label = st.selectbox("Label (0/1)", options=[c for c in all_cols if c in ("high_tip","label","y")], index=0)
#         defaults = [c for c in ["geo_distance","duration_min","speed_mph","hour","is_weekend","passenger_count","fare_amount"] if c in all_cols]
#         feats = st.multiselect("Features", options=numeric_cols, default=defaults)
#         test_size = st.slider("Test fraction", 0.1, 0.5, 0.2, 0.05)
#         thr = st.slider("Decision threshold (probability of class 1)", 0.1, 0.9, 0.5, 0.05)
# 
#         if st.button("Train classification"):
#             if not feats:
#                 st.warning("Select at least one feature.")
#             else:
#                 try:
#                     feats = [c for c in feats if c != label]
#                     df_ml = clean_numeric(df_eng, list(set(feats + [label])))
#                     # remove any leftover columns from previous runs
#                     df_ml = drop_if_exists(df_ml, ["prediction","rawPrediction","probability"])
# 
#                     imputed = [f"{c}_imp" for c in feats]
#                     pred_col = f"pred_{label}"
#                     prob_col = f"prob_{label}"
#                     raw_col  = f"rawPred_{label}"
# 
#                     logit = LogisticRegression(featuresCol="features", labelCol=label, maxIter=50,
#                                                predictionCol=pred_col, probabilityCol=prob_col, rawPredictionCol=raw_col)
# 
#                     pipe = Pipeline(stages=[
#                         Imputer(inputCols=feats, outputCols=imputed, strategy="median"),
#                         VectorAssembler(inputCols=imputed, outputCol="features"),
#                         logit
#                     ])
#                     data = df_ml.filter(col(label).isNotNull())
#                     train, test = data.randomSplit([1.0 - test_size, test_size], seed=7)
#                     fitted = pipe.fit(train)
#                     preds  = fitted.transform(test)
# 
#                     # Metrics at model's default threshold
#                     auc = BinaryClassificationEvaluator(labelCol=label, rawPredictionCol=raw_col).evaluate(preds)
#                     acc = MulticlassClassificationEvaluator(labelCol=label, predictionCol=pred_col, metricName="accuracy").evaluate(preds)
#                     st.success(f"AUC: {auc:.3f} | ACC: {acc:.3f}")
# 
#                     # ROC curve
#                     fig_roc = plot_roc_from_preds(preds, label, prob_col)
#                     st.plotly_chart(fig_roc, use_container_width=True)
#                     st.download_button("â¬‡ï¸ Chart (HTML): ROC",
#                                        fig_to_html_bytes(fig_roc), "roc_curve.html", "text/html")
# 
#                     # Thresholded predictions for confusion matrix & PR/F1
#                     pred_thr_col = f"{pred_col}_thr"
#                     preds = preds.withColumn(pred_thr_col, (F.col(prob_col).getItem(1) >= F.lit(float(thr))).cast("int"))
# 
#                     cm_pd = preds.select(label, pred_thr_col).toPandas()
#                     cm = pd.crosstab(cm_pd[label], cm_pd[pred_thr_col], rownames=["Actual"], colnames=["Predicted"], dropna=False)
#                     TP = cm.get(1, pd.Series()).get(1, 0)
#                     FP = cm.get(1, pd.Series()).get(0, 0)
#                     FN = cm.get(0, pd.Series()).get(1, 0)
#                     precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
#                     recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0
#                     f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
#                     st.write(f"Threshold = **{thr:.2f}**  â†’  Precision: **{precision:.3f}**, Recall: **{recall:.3f}**, F1: **{f1:.3f}**")
#                     st.write("Confusion matrix (with threshold)"); st.dataframe(cm)
# 
#                     # Predictions table + download
#                     out_pd = preds.select(*feats, label, pred_col, prob_col, pred_thr_col).limit(5000).toPandas()
#                     st.dataframe(out_pd)
#                     st.download_button("â¬‡ï¸ Predictions (CSV)",
#                                        out_pd.to_csv(index=False).encode(),
#                                        "classification_predictions.csv", "text/csv")
#                 except Exception as e:
#                     st.error(f"Classification error: {e}")
# 
# # ---- 4) (Optional) Clustering demo ----
# st.subheader("ðŸ“ KMeans on pickup locations (optional)")
# if df_eng is not None and {"pickup_longitude","pickup_latitude"}.issubset(df_eng.columns):
#     k = st.slider("K (clusters)", 2, 12, 6, 1)
#     frac = st.slider("Sample fraction for KMeans", 0.01, 0.20, 0.05, 0.01)
#     if st.button("Run KMeans"):
#         try:
#             pts = (df_eng
#                    .select(F.col("pickup_longitude").cast("double").alias("lon"),
#                            F.col("pickup_latitude").cast("double").alias("lat"))
#                    .dropna()
#                    .filter(F.col("lon").between(-75, -72))
#                    .filter(F.col("lat").between(40, 42))
#                    .sample(False, float(frac), seed=123))
#             km_va = VectorAssembler(inputCols=["lon","lat"], outputCol="features")
#             km_model = KMeans(k=int(k), seed=123).fit(km_va.transform(pts))
#             centers = [tuple(c.tolist()) for c in km_model.clusterCenters()]
#             st.success(f"Centers (lon, lat): {centers}")
# 
#             pts_pd = pts.limit(4000).toPandas()
#             fig = px.scatter(pts_pd, x="lon", y="lat", title="Pickup points (sample)")
#             if centers:
#                 cx, cy = zip(*centers)
#                 fig.add_trace(go.Scatter(x=cx, y=cy, mode="markers+text",
#                                          text=[f"C{i}" for i in range(len(centers))],
#                                          textposition="top center", marker=dict(size=12),
#                                          name="Centers"))
#             st.plotly_chart(fig, use_container_width=True)
#             st.download_button("â¬‡ï¸ Chart (HTML): KMeans", fig_to_html_bytes(fig), "kmeans_map.html", "text/html")
#         except Exception as e:
#             st.error(f"KMeans error: {e}")
# 
# st.caption("Tip: SQL table is `data`. Engineered columns: hour, dow, is_weekend, geo_distance, duration_min, speed_mph.")
#

# Install once per runtime
!pip -q install streamlit==1.36.0 watchdog==4.0.1

# Kill any previous instance
!pkill -f streamlit -9 || echo "no prior streamlit"

PORT=8501
!python -m streamlit run /content/app.py --server.port $PORT --server.headless true \
  --server.address 0.0.0.0 --server.enableCORS false --server.enableXsrfProtection false \
  --browser.gatherUsageStats false --server.fileWatcherType none &> /content/streamlit.log &

from time import sleep
from google.colab import output
sleep(3)
print("âœ… Open this:", output.eval_js(f'google.colab.kernel.proxyPort({PORT}, "https")'))
!tail -n 40 /content/streamlit.log